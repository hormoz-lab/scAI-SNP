{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "written-hypothesis",
   "metadata": {},
   "source": [
    "# 1. set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519726ce",
   "metadata": {},
   "source": [
    "## 1.1. libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"print version\")\n",
    "print(sys.version)\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math # for plotting network\n",
    "import matplotlib.gridspec as gridspec # for plotting figure 3\n",
    "import matplotlib.patches as mpatches # for plotting network\n",
    "from matplotlib.lines import Line2D # for plotting\n",
    "from matplotlib.patches import Circle # for plotting network\n",
    "from matplotlib.patches import Patch # for plotting network\n",
    "from matplotlib.patches import Rectangle # for plotting figure 5\n",
    "import matplotlib.patches as patches # for plotting figure\n",
    "\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import cvxpy as cp\n",
    "\n",
    "from helper_plot import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-lending",
   "metadata": {},
   "source": [
    "## 1.2. load up variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c44a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_project = '../'\n",
    "path_mat_PC_train = f'{path_project}data/mat_PC/train/'\n",
    "path_mat_PC_test = f'{path_project}data/mat_PC/test/'\n",
    "path_label = f'{path_project}data/label/'\n",
    "path_population = f'{path_project}data/population/'\n",
    "path_plot = f'{path_project}figure/'\n",
    "\n",
    "df_meta = pd.read_csv(path_population + 'meta_merged.csv')\n",
    "df_desc = pd.read_csv(path_population + 'desc_subpopulation.tsv', delimiter = '\\t')\n",
    "df_desc = df_desc[['Population code', 'Population description']]\n",
    "df_desc.set_index('Population code', inplace = True)\n",
    "\n",
    "dict_desc = df_desc['Population description'].to_dict()\n",
    "n_PC = 600\n",
    "\n",
    "n_train = 2560\n",
    "n_test = 641\n",
    "n_subpop = 26\n",
    "\n",
    "mat_GT_train = np.loadtxt(f'{path_mat_PC_train}mat_PC_train.tsv', delimiter = '\\t')[:, 0:n_PC]\n",
    "\n",
    "vec_label_train_index = np.loadtxt(f'{path_label}index_train.csv', delimiter = ',')\n",
    "vec_label_train_index = vec_label_train_index.astype(int) - 4\n",
    "vec_label_test_index = np.loadtxt(f'{path_label}index_test.csv', delimiter = ',')\n",
    "vec_label_test_index = vec_label_test_index.astype(int) - 4\n",
    "\n",
    "vec_class = pd.read_csv(f'{path_population}population.tsv', delimiter = '\\t', header = None)\n",
    "vec_class = np.array(vec_class.values).flatten()\n",
    "\n",
    "df_label = pd.read_csv(f'{path_population}meta_merged.csv', delimiter = ',')\n",
    "\n",
    "vec_label_test = [df_label['SUP'][i] for i in vec_label_test_index]\n",
    "vec_label_train = [df_label['SUP'][i] for i in vec_label_train_index]\n",
    "\n",
    "dict_mean_GT_subpop = {}\n",
    "mat_mean_GT_subpop = np.zeros([n_PC, 26])\n",
    "for i, cls in enumerate(vec_class):\n",
    "    vec_index_subpop = [i for i in range(n_train) if vec_label_train[i] == cls]\n",
    "    mat_mean_GT_subpop[:, i] = sum(mat_GT_train[vec_index_subpop, :]) / len(vec_index_subpop)\n",
    "    dict_mean_GT_subpop[cls] = mat_mean_GT_subpop[:, i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb37da0",
   "metadata": {},
   "source": [
    "## 1.3. functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92762eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_convex(vec_test, label_test, mat_mean_GT_subpop):\n",
    "    # Define the optimization variable\n",
    "    X = cp.Variable(n_subpop)\n",
    "\n",
    "    # Define the objective function (minimize the least squares error)\n",
    "    objective = cp.Minimize(cp.norm(mat_mean_GT_subpop @ X - vec_test, 'fro'))\n",
    "\n",
    "    # Define the constraints\n",
    "    constraints = [X >= 0, cp.sum(X) == 1]\n",
    "\n",
    "    # Define and solve the problem\n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve()\n",
    "\n",
    "    result_pred = X.value\n",
    "    result_pred_subpop = vec_class[np.argmax(result_pred)]\n",
    "    \n",
    "    # Use result_pred to construct a n_PC-dimensional vector made from linear combinations of the mean vectors\n",
    "    result_pred_PC = mat_mean_GT_subpop @ result_pred\n",
    "\n",
    "    mean_PC_subpop = dict_mean_GT_subpop[label_test]\n",
    "\n",
    "    # Compute the cosine similarity between result_pred_PC and vec_test\n",
    "    cos_sim = np.dot(result_pred_PC, vec_test) / (np.linalg.norm(result_pred_PC) * np.linalg.norm(vec_test))\n",
    "    \n",
    "    # Compute the cosine similarity between mean_PC_subpop and vec_test\n",
    "    cos_sim_subpop = np.dot(mean_PC_subpop, vec_test) / (np.linalg.norm(mean_PC_subpop) * np.linalg.norm(vec_test))\n",
    "\n",
    "    bool_correct = (result_pred_subpop == label_test)\n",
    "\n",
    "    return result_pred, result_pred_subpop, cos_sim, cos_sim_subpop, bool_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d2b21",
   "metadata": {},
   "source": [
    "# 2. confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f0e02c",
   "metadata": {},
   "source": [
    "## 2.1. train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d6f5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vec_name_supp_fig1 = ['No', '90.0%', '99.0%', '99.9%'] # DELETE\n",
    "# takes about 6.5 minutes\n",
    "vec_name_supp_fig1 = ['full', '500k', '50k', '5k']\n",
    "\n",
    "dict_list_pred = {}\n",
    "for test_i in vec_name_supp_fig1:\n",
    "    print(f'starting with iteration {test_i}')\n",
    "    df_result = np.zeros([n_test, 5])\n",
    "    df_result = pd.DataFrame(df_result)\n",
    "\n",
    "    mat_GT_test = np.loadtxt(f'{path_mat_PC_test}mat_PC_test_{test_i}.tsv', delimiter = '\\t')[0:n_PC, :]\n",
    "    mat_GT_test = np.transpose(mat_GT_test)\n",
    "\n",
    "    for index_test in range(n_test):\n",
    "        \n",
    "        result_pred, result_pred_subpop, cos_sim, cos_sim_subpop, bool_correct = pred_convex(mat_GT_test[index_test, :], vec_label_test[index_test], mat_mean_GT_subpop)\n",
    "        df_result.iloc[index_test, :] = [vec_label_test[index_test], result_pred_subpop, bool_correct, cos_sim, cos_sim_subpop]\n",
    "    \n",
    "    dict_list_pred[test_i] = df_result[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4118d094",
   "metadata": {},
   "source": [
    "## 2.2. report accuracies per scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb6bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(df_result[0] == dict_list_pred['full'])/n_test)\n",
    "print(sum(df_result[0] == dict_list_pred['500k'])/n_test)\n",
    "print(sum(df_result[0] == dict_list_pred['50k'])/n_test)\n",
    "print(sum(df_result[0] == dict_list_pred['5k'])/n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23874dcb",
   "metadata": {},
   "source": [
    "## 2.3. plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2950a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'train_test'\n",
    "n_col = 2\n",
    "n_plot = 2\n",
    "size_font_small = 5\n",
    "vec_name_fig5 = ['No', '90%', '99%', '99.9%']\n",
    "\n",
    "# Convert mm to inches for figsize\n",
    "width_in_inches = 170 / 25.4\n",
    "height_in_inches = 220 / 25.4\n",
    "\n",
    "# Create the figure with specified size\n",
    "fig, axs = plt.subplots(ncols = n_col, nrows = 2,\n",
    "                        figsize=(width_in_inches, height_in_inches), sharex=True, sharey=True)\n",
    "\n",
    "for i, i_plot in enumerate(range(4)):\n",
    "    \n",
    "    ax_i = axs[0 if i_plot <= 1 else 1, 0 if i_plot % 2 == 0 else 1]\n",
    "    # ax_i = axs[i]\n",
    "    \n",
    "    # Predict the labels for the test set\n",
    "    y_pred = dict_list_pred[vec_name_supp_fig1[i]]\n",
    "\n",
    "    # Compute the accuracy\n",
    "    score_acc = round(accuracy_score(vec_label_test, y_pred)*100, 2)\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(vec_label_test, y_pred, labels = list(vec_label_sorted))\n",
    "    df_cm = pd.DataFrame(cm, columns = vec_label_sorted, index = vec_label_sorted)\n",
    "    \n",
    "    # annotation array to not annotate 0\n",
    "    annot_array = np.array([['' if i == 0 else str(i) for i in inner] for inner in df_cm.values])\n",
    "    \n",
    "    ax_i = sns.heatmap(df_cm, \n",
    "                annot = annot_array, \n",
    "                annot_kws = {\"size\": size_font_small}, \n",
    "                fmt='', \n",
    "                cmap='Greys',  # Change to grayscale\n",
    "                ax = ax_i, \n",
    "                cbar = True, \n",
    "                vmin = 0, \n",
    "                vmax = 45)\n",
    "    \n",
    "    # Set the font size for colorbar labels\n",
    "    cbar = ax_i.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=size_font_small)\n",
    "    \n",
    "    ax_i.set_aspect('equal', 'box')\n",
    "    ax_i.set_title(\"Test Data with \" + vec_name_fig5[i] + \" Missing (accuary: \" + str(score_acc) + \"%)\", \n",
    "                   fontsize = size_font_small)\n",
    "    \n",
    "    # Set x-axis tick positions and labels\n",
    "    ax_i.set_xticks([x - 0.5 for x in range(1, len(vec_label_sorted) + 1)])\n",
    "    ax_i.set_xticklabels(vec_label_sorted, fontsize=size_font_small, ha = 'center')\n",
    "\n",
    "    # Set y-axis tick positions and labels\n",
    "    ax_i.set_yticks([y - 0.5 for y in range(1, len(vec_label_sorted) + 1)])\n",
    "    ax_i.set_yticklabels(vec_label_sorted, fontsize=size_font_small)\n",
    "    \n",
    "    ax_i.tick_params(length = 0) # remove tick marks\n",
    "    \n",
    "    # Set black gridlines around the plot area\n",
    "    for _, spine in ax_i.spines.items():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_edgecolor('black')\n",
    "        spine.set_linewidth(1)\n",
    "        \n",
    "    # Draw grey gridlines at specific x-axis locations\n",
    "    # Assuming 'CLM' and 'CDX' are in your column labels\n",
    "    line_positions = [ax_i.get_xticklabels().index(label) for label in ax_i.get_xticklabels() if label.get_text() in ['CLM', 'CDX', 'CEU', 'BEB']]\n",
    "    for pos in line_positions:\n",
    "        ax_i.axvline(pos, color='grey', linestyle='--', linewidth=0.5)\n",
    "        ax_i.axhline(pos, color='grey', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    for label in (ax_i.get_xticklabels() + ax_i.get_yticklabels()):\n",
    "        pop_temp = label.get_text()\n",
    "        color_temp = dict_color_super[df_meta_unique[df_meta_unique['SUP'] == pop_temp]['POP'].values[0]]\n",
    "        label.set_bbox(dict(facecolor = color_temp, \n",
    "                            edgecolor='None', \n",
    "                            alpha = 0.5, pad = 1))\n",
    "        \n",
    "    # rectangles to show super population\n",
    "    index_rect = 0\n",
    "    for sup_i in vec_sup:\n",
    "        ax_i.add_patch(Rectangle((index_rect, index_rect), \n",
    "                                 dict_sup_total[sup_i], dict_sup_total[sup_i], \n",
    "                                 fill = False, edgecolor = dict_color_super[sup_i], lw = 2))\n",
    "        index_rect += dict_sup_total[sup_i]\n",
    "        \n",
    "    # Create custom legend\n",
    "    legend_elements = [Line2D([0], [0], linestyle='-', color=color, label=continent) \n",
    "                       for continent, color in dict_color_super.items()]\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "fig.text(0.5, -0.01, 'Group Classification (Model)', ha = 'center', va = 'center', fontsize = size_font_small)\n",
    "fig.text(-0.01, 0.5, 'Group Classification (Truth)', ha = 'center', va = 'center', rotation = 'vertical', fontsize = size_font_small)\n",
    "plt.subplots_adjust(top = 0.82)\n",
    "fig.savefig(fn_ensure_slash(path_plot) + 'sfigure1/figure_supp_confusion_matrix_four.pdf', format = 'pdf', dpi = 1200)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
